{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DynUNet on Decathlon datasets\n",
    "This tutorial shows how to train 3D segmentation tasks on all the 10 decathlon datasets with `DynUNet`.\n",
    "\n",
    "Refer to papers:\n",
    "\n",
    "`Automated Design of Deep Learning Methods for Biomedical Image Segmentation <https://arxiv.org/abs/1904.08128>`\n",
    "\n",
    "`nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation <https://arxiv.org/abs/1809.10486>`\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/master/modules/dynunet_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai[nibabel, ignite, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: install MONAI from source code, will remove it when v0.5.0 released\n",
    "%pip install git+https://github.com/Project-MONAI/MONAI#egg=MONAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from monai.engines import SupervisedEvaluator, SupervisedTrainer\n",
    "from monai.handlers import (\n",
    "    CheckpointSaver,\n",
    "    LrScheduleHandler,\n",
    "    MeanDice,\n",
    "    StatsHandler,\n",
    "    ValidationHandler,\n",
    ")\n",
    "from monai.inferers import SimpleInferer, SlidingWindowInferer\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.networks.nets import DynUNet\n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    AsDiscreted,\n",
    "    CastToTyped,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandFlipd,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandScaleIntensityd,\n",
    "    RandZoomd,\n",
    "    Spacingd,\n",
    "    SpatialPadd,\n",
    "    ToTensord,\n",
    ")\n",
    "from torch.nn.functional import interpolate\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Decathlon task\n",
    "The Decathlon dataset contains 10 tasks, this dynUNet tutorial can support all of them.\n",
    "\n",
    "Just need to select task ID and other parameters will be automatically selected.\n",
    "\n",
    "(Tested task 04 locally, epoch time is 8 secs on V100 GPU and best metrics is 0.8828 at epoch: 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = \"04\"\n",
    "\n",
    "task_name = {\n",
    "    \"01\": \"Task01_BrainTumour\",\n",
    "    \"02\": \"Task02_Heart\",\n",
    "    \"03\": \"Task03_Liver\",\n",
    "    \"04\": \"Task04_Hippocampus\",\n",
    "    \"05\": \"Task05_Prostate\",\n",
    "    \"06\": \"Task06_Lung\",\n",
    "    \"07\": \"Task07_Pancreas\",\n",
    "    \"08\": \"Task08_HepaticVessel\",\n",
    "    \"09\": \"Task09_Spleen\",\n",
    "    \"10\": \"Task10_Colon\",\n",
    "}\n",
    "\n",
    "patch_size = {\n",
    "    \"01\": [128, 128, 128],\n",
    "    \"02\": [160, 192, 80],\n",
    "    \"03\": [128, 128, 128],\n",
    "    \"04\": [40, 56, 40],\n",
    "    \"05\": [320, 256, 20],\n",
    "    \"06\": [192, 160, 80],\n",
    "    \"07\": [224, 224, 40],\n",
    "    \"08\": [192, 192, 64],\n",
    "    \"09\": [192, 160, 64],\n",
    "    \"10\": [192, 160, 56],\n",
    "}\n",
    "\n",
    "spacing = {\n",
    "    \"01\": [1.0, 1.0, 1.0],\n",
    "    \"02\": [1.25, 1.25, 1.37],\n",
    "    \"03\": [0.77, 0.77, 1],\n",
    "    \"04\": [1.0, 1.0, 1.0],\n",
    "    \"05\": [0.62, 0.62, 3.6],\n",
    "    \"06\": [0.79, 0.79, 1.24],\n",
    "    \"07\": [0.8, 0.8, 2.5],\n",
    "    \"08\": [0.8, 0.8, 1.5],\n",
    "    \"09\": [0.79, 0.79, 1.6],\n",
    "    \"10\": [0.78, 0.78, 3],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
    "This allows you to save results and reuse downloads.  \n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train and validation transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        AddChanneld(keys=[\"image\", \"label\"]),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=spacing[task_id],\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        SpatialPadd(keys=[\"image\", \"label\"], spatial_size=patch_size[task_id]),\n",
    "        NormalizeIntensityd(keys=[\"image\"], nonzero=False, channel_wise=True),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            label_key=\"label\",\n",
    "            spatial_size=patch_size[task_id],\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=1,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "        RandZoomd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            min_zoom=0.9,\n",
    "            max_zoom=1.2,\n",
    "            mode=(\"trilinear\", \"nearest\"),\n",
    "            align_corners=(True, None),\n",
    "            prob=0.16,\n",
    "        ),\n",
    "        CastToTyped(keys=[\"image\", \"label\"], dtype=(np.float32, np.uint8)),\n",
    "        RandGaussianNoised(keys=[\"image\"], std=0.01, prob=0.15),\n",
    "        RandGaussianSmoothd(\n",
    "            keys=[\"image\"],\n",
    "            sigma_x=(0.5, 1.15),\n",
    "            sigma_y=(0.5, 1.15),\n",
    "            sigma_z=(0.5, 1.15),\n",
    "            prob=0.15,\n",
    "        ),\n",
    "        RandScaleIntensityd(keys=[\"image\"], factors=0.3, prob=0.15),\n",
    "        RandFlipd([\"image\", \"label\"], spatial_axis=[0, 1, 2], prob=0.5),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        AddChanneld(keys=[\"image\", \"label\"]),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=spacing[task_id],\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        SpatialPadd(keys=[\"image\", \"label\"], spatial_size=patch_size[task_id]),\n",
    "        NormalizeIntensityd(keys=[\"image\"], nonzero=False, channel_wise=True),\n",
    "        CastToTyped(keys=[\"image\", \"label\"], dtype=(np.float32, np.uint8)),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data by MONAI DecathlonDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=task_name[task_id],\n",
    "    transform=train_transform,\n",
    "    section=\"training\",\n",
    "    download=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=1)\n",
    "\n",
    "val_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=task_name[task_id],\n",
    "    transform=val_transform,\n",
    "    section=\"validation\",\n",
    "    download=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize batch of data to check images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    image, label = val_ds[i][\"image\"], val_ds[i][\"label\"]\n",
    "    plt.figure(\"check\", (12, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"image\")\n",
    "    plt.imshow(image[0, :, :, 10].detach().cpu(), cmap=\"gray\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"label\")\n",
    "    plt.imshow(label[0, :, :, 10].detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize training components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "loss = DiceCELoss(to_onehot_y=True, softmax=True, batch=False)\n",
    "learning_rate = 0.01\n",
    "max_epochs = 200\n",
    "\n",
    "sizes, spacings = patch_size[task_id], spacing[task_id]\n",
    "properties = val_ds.get_properties(keys=[\"labels\", \"modality\"])\n",
    "n_class, in_channels = len(properties[\"labels\"]), len(properties[\"modality\"])\n",
    "best_dice, best_epoch = (n_class - 1) * [0], (n_class - 1) * [0]\n",
    "strides, kernels = [], []\n",
    "\n",
    "while True:\n",
    "    spacing_ratio = [sp / min(spacings) for sp in spacings]\n",
    "    stride = [\n",
    "        2 if ratio <= 2 and size >= 8 else 1\n",
    "        for (ratio, size) in zip(spacing_ratio, sizes)\n",
    "    ]\n",
    "    kernel = [3 if ratio <= 2 else 1 for ratio in spacing_ratio]\n",
    "    if all(s == 1 for s in stride):\n",
    "        break\n",
    "    sizes = [i / j for i, j in zip(sizes, stride)]\n",
    "    spacings = [i * j for i, j in zip(spacings, stride)]\n",
    "    kernels.append(kernel)\n",
    "    strides.append(stride)\n",
    "strides.insert(0, len(spacings) * [1])\n",
    "kernels.append(len(spacings) * [3])\n",
    "net = DynUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=in_channels,\n",
    "    out_channels=n_class,\n",
    "    kernel_size=kernels,\n",
    "    strides=strides,\n",
    "    upsample_kernel_size=strides[1:],\n",
    "    norm_name=\"instance\",\n",
    "    deep_supervision=True,\n",
    "    deep_supr_num=2,\n",
    "    res_block=False,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.95)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lr_lambda=lambda epoch: (1 - epoch / max_epochs) ** 0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONAI evaluator\n",
    "Here we customized the forward computation, so need to define `_iteration` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_handlers = [\n",
    "    StatsHandler(output_transform=lambda x: None),\n",
    "    CheckpointSaver(\n",
    "        save_dir=\"./runs/\", save_dict={\"net\": net}, save_key_metric=True\n",
    "    ),\n",
    "]\n",
    "\n",
    "val_post_transform = Compose(\n",
    "    [\n",
    "        AsDiscreted(\n",
    "            keys=(\"pred\", \"label\"),\n",
    "            argmax=(True, False),\n",
    "            to_onehot=True,\n",
    "            n_classes=n_class,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define customized evaluator\n",
    "\n",
    "\n",
    "class DynUNetEvaluator(SupervisedEvaluator):\n",
    "    def _iteration(self, engine, batchdata):\n",
    "        inputs, targets = self.prepare_batch(batchdata)\n",
    "        inputs, targets = inputs.to(engine.state.device), targets.to(\n",
    "            engine.state.device\n",
    "        )\n",
    "        flip_inputs = torch.flip(inputs, dims=(2, 3, 4))\n",
    "\n",
    "        def _compute_pred():\n",
    "            pred = self.inferer(inputs, self.network)\n",
    "            flip_pred = torch.flip(\n",
    "                self.inferer(flip_inputs, self.network), dims=(2, 3, 4)\n",
    "            )\n",
    "            return (pred + flip_pred) / 2\n",
    "\n",
    "        # execute forward computation\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            if self.amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    predictions = _compute_pred()\n",
    "            else:\n",
    "                predictions = _compute_pred()\n",
    "        return {\"image\": inputs, \"label\": targets, \"pred\": predictions}\n",
    "\n",
    "\n",
    "evaluator = DynUNetEvaluator(\n",
    "    device=device,\n",
    "    val_data_loader=val_loader,\n",
    "    network=net,\n",
    "    inferer=SlidingWindowInferer(\n",
    "        roi_size=patch_size[task_id], sw_batch_size=4, overlap=0.5\n",
    "    ),\n",
    "    post_transform=val_post_transform,\n",
    "    key_val_metric={\n",
    "        \"val_mean_dice\": MeanDice(\n",
    "            include_background=False,\n",
    "            output_transform=lambda x: (x[\"pred\"], x[\"label\"]),\n",
    "        )\n",
    "    },\n",
    "    val_handlers=val_handlers,\n",
    "    amp=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## MONAI trainer\n",
    "Here we customized loss computation progress, so need to define `_iteration` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_handlers = [\n",
    "    LrScheduleHandler(lr_scheduler=scheduler, print_lr=True),\n",
    "    ValidationHandler(validator=evaluator, interval=2, epoch_level=True),\n",
    "    StatsHandler(tag_name=\"train_loss\", output_transform=lambda x: x[\"loss\"]),\n",
    "]\n",
    "\n",
    "# define customized trainer\n",
    "\n",
    "\n",
    "class DynUNetTrainer(SupervisedTrainer):\n",
    "    def _iteration(self, engine, batchdata):\n",
    "        inputs, targets = self.prepare_batch(batchdata)\n",
    "        inputs, targets = inputs.to(engine.state.device), targets.to(\n",
    "            engine.state.device\n",
    "        )\n",
    "\n",
    "        def _compute_loss(preds, label):\n",
    "            preds = torch.unbind(preds, dim=1)\n",
    "            return sum(\n",
    "                [\n",
    "                    0.5 ** i * self.loss_function.forward(p, label)\n",
    "                    for i, p in enumerate(preds)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.network.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        if self.amp and self.scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                predictions = self.inferer(inputs, self.network)\n",
    "                loss = _compute_loss(predictions, targets)\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            predictions = self.inferer(inputs, self.network)\n",
    "            loss = _compute_loss(predictions, targets).mean()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return {\n",
    "            \"image\": inputs,\n",
    "            \"label\": targets,\n",
    "            \"pred\": predictions,\n",
    "            \"loss\": loss.item(),\n",
    "        }\n",
    "\n",
    "\n",
    "trainer = DynUNetTrainer(\n",
    "    device=device,\n",
    "    max_epochs=max_epochs,\n",
    "    train_data_loader=train_loader,\n",
    "    network=net,\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss,\n",
    "    inferer=SimpleInferer(),\n",
    "    post_transform=None,\n",
    "    key_train_metric=None,\n",
    "    train_handlers=train_handlers,\n",
    "    amp=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training with workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
